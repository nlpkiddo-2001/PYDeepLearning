# Pre-training Configuration
# Target: ~1.1B Parameters

stage: "pretrain"

data_dir: "./data/pretraining"
output_dir: "./checkpoints/pretrain"

resume_from: null

model: 
  dim: 2048                     
  n_layers: 20                  
  n_heads: 32                   
  n_kv_heads: null
  multiple_of: 256
  ffn_dim_multiplier: null
  norm_eps: 1.0e-5
  dropout: 0.0
  gradient_checkpointing: true

batch_size: 12                  
grad_accum: 8               
max_steps: 1000
warmup_steps: 20
learning_rate: 6.0e-4            
lr_min: 6.0e-5
context_window: 4096
save_every: 5000

use_muon: false

optimization:
  scheduler_type: "cosine" 
  
  batch_size_schedule: null

  context_length_schedule: null