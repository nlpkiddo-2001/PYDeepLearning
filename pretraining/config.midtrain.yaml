# Mid-training Configuration
# Use this for continued training on domain-specific or higher-quality data

stage: "midtrain"

data_dir: "./data/midtraining"
output_dir: "./checkpoints/midtrain"

resume_from: "auto"              


model: 
  dim: 2048                     
  n_layers: 20                  
  n_heads: 32                   
  n_kv_heads: null
  multiple_of: 256
  ffn_dim_multiplier: null
  norm_eps: 1.0e-5
  dropout: 0.0


batch_size: 12                  
grad_accum: 8                    
max_steps: 500               
warmup_steps: 30                
learning_rate: 3.0e-4            
lr_min: 3.0e-5                  
context_window: 4096             
save_every: 2000                

use_muon: false

optimization:
  scheduler_type: "wsd" 
  

  batch_size_schedule: null
    
  context_length_schedule:
    - [0, 4096]
    - [5000, 8192]
    - [10000, 16384]

